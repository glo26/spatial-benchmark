\documentclass{article}

\usepackage{neurips_2023}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor}

\hypersetup{colorlinks=false,allcolors=black,linkcolor=black,citecolor=black,urlcolor=black}

\title{SpatialOps: A Benchmark for 2D Spatial Planning and Reasoning in Large Language Models}

\author{Anonymous Author(s)}

\begin{document}

\maketitle

\begin{abstract}
Spatial reasoning represents a fundamental cognitive capability that enables humans to navigate, plan, and interact with the physical world. Despite remarkable advances in Large Language Models (LLMs), their ability to perform spatial reasoning remains significantly limited compared to their linguistic capabilities. Existing benchmarks have explored various facets of spatial understanding, yet a comprehensive evaluation framework for practical 2D spatial planning across diverse real-world domains is notably absent. We introduce \textbf{SpatialOps}, a comprehensive benchmark comprising 6,012 procedurally generated tasks across twelve categories organized into three tiers of increasing complexity. Our benchmark uniquely bridges the gap between abstract spatial reasoning and applied operational planning, drawing from documented use cases in telecommunications, utilities, government, and enterprise sectors. We propose a multi-faceted evaluation methodology encompassing five metrics: Task Completion Rate, Human-AI Latency Ratio, Operational Cost Savings, Efficacy Score, and Scalability Index. Extensive experiments on five leading LLMs reveal substantial performance gaps, with the best model achieving only 78.4\% on our composite score. Our analysis identifies systematic weaknesses in algorithmic reasoning, constraint satisfaction, and temporal-spatial integration, providing clear directions for future research.
\end{abstract}

\section{Introduction}

The emergence of Large Language Models has fundamentally transformed artificial intelligence, demonstrating unprecedented capabilities in natural language understanding \cite{brown2020gpt3, touvron2023llama, anil2023palm}, code generation \cite{chen2021evaluating, li2022competition}, and complex reasoning \cite{wei2022chain, yao2023tree, kojima2022large}. These models have shown remarkable performance on tasks ranging from mathematical problem-solving \cite{hendrycks2021measuring, cobbe2021training} to scientific discovery \cite{romera2024mathematical, trinh2024solving}. However, a critical examination of their capabilities reveals a fundamental limitation: the ability to reason about spatial relationships and perform spatial planning remains significantly underdeveloped \cite{liu2023agentbench, bang2023multitask, srivastava2022beyond}.

This limitation is particularly consequential given the central role that spatial reasoning plays in human cognition \cite{newcombe2010picture, hegarty2006spatial, uttal2013malleability}. From navigating through physical environments \cite{wolbers2010determines, ekstrom2014cellular} to understanding maps and diagrams \cite{hegarty2004diagrams, tversky2005functional}, spatial reasoning underpins countless everyday activities and professional tasks. The cognitive science literature has long recognized spatial ability as a distinct form of intelligence \cite{carroll1993human, mcgee1979human}, separate from verbal and mathematical reasoning, and critical for success in STEM fields \cite{wai2009spatial, uttal2013malleability}.

The challenge of spatial reasoning for LLMs stems from a fundamental representational mismatch \cite{bisk2020experience, patel2021mapping}. These models process information as discrete, sequential tokens, whereas spatial information is inherently continuous and multi-dimensional \cite{forbus1984qualitative, kuipers1978modeling}. Early work in qualitative spatial reasoning established formal frameworks for representing spatial relationships \cite{randell1992spatial, cohn1997qualitative, egenhofer1991reasoning}, but translating these frameworks into neural architectures remains an open challenge \cite{chen2024spatialreasoning, mirzaee2021spartqa}.

The practical implications of this limitation are substantial. As AI systems are increasingly deployed in real-world applications, from autonomous vehicles \cite{chen2015deepdriving, bojarski2016endtoend, pomerleau1988alvinn} to robotic manipulation \cite{levine2016end, kalashnikov2018scalable, zeng2018learning}, the ability to reason spatially becomes critical. In enterprise contexts, spatial AI is transforming industries including telecommunications \cite{zhang2019deep, wang2020deep}, urban planning \cite{batty2013big, bibri2017smart}, logistics \cite{li2019learning, nazari2018reinforcement}, and real estate \cite{law2019take, fu2019real}. Companies like Palantir \cite{palantir2024maven}, Scale AI \cite{scaleai2025donovan}, Wherobots \cite{wherobots2026sedona}, and Google Earth Engine \cite{google2025earthengine} are deploying sophisticated spatial AI systems, yet the underlying LLMs that power many of these applications lack robust spatial reasoning capabilities.

To address this gap, we introduce \textbf{SpatialOps}, a comprehensive benchmark designed to evaluate the 2D spatial planning and reasoning capabilities of LLMs. Our benchmark makes four key contributions:

\begin{enumerate}
    \item \textbf{Comprehensive Task Coverage:} We define twelve distinct task categories spanning three tiers of complexity, from foundational concepts like coordinate understanding and distance computation to advanced optimization problems involving constraint satisfaction and temporal-spatial reasoning.
    
    \item \textbf{Real-World Grounding:} Unlike abstract benchmarks, SpatialOps is grounded in documented industry use cases from telecommunications, utilities, government, and enterprise sectors, ensuring practical relevance.
    
    \item \textbf{Rigorous Evaluation Methodology:} We propose five complementary metrics that assess not only accuracy but also efficiency, cost-effectiveness, and scalability, providing a holistic view of model capabilities.
    
    \item \textbf{Extensive Empirical Analysis:} We evaluate five leading LLMs, conduct ablation studies on prompt engineering and task complexity, and provide detailed error analysis to guide future research.
\end{enumerate}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/figure1_framework.png}
\caption{The SpatialOps benchmark framework. Tasks span twelve categories organized into three tiers of increasing complexity. Models are evaluated using a multi-faceted methodology that assesses accuracy, reasoning quality, and operational efficiency.}
\label{fig:framework}
\end{center}
\end{figure}

\section{Related Work}

\subsection{Spatial Reasoning in Cognitive Science}

The study of spatial reasoning has deep roots in cognitive psychology and neuroscience. Piaget's foundational work established that spatial cognition develops through distinct stages \cite{piaget1956child}, while subsequent research identified multiple components of spatial ability including mental rotation \cite{shepard1971mental, vandenberg1978mental}, spatial visualization \cite{lohman1979spatial, hegarty2004diagrams}, and spatial orientation \cite{kozhevnikov2006spatial, hegarty2002individual}. Neuroimaging studies have localized spatial processing to specific brain regions, particularly the parietal cortex and hippocampus \cite{burgess2002spatial, kravitz2011new, epstein2017cognitive}.

The distinction between egocentric and allocentric spatial reference frames \cite{klatzky1998allocentric, burgess2006spatial} has proven particularly relevant for AI systems. Egocentric representations encode space relative to the observer, while allocentric representations use external reference points. Research suggests that humans flexibly switch between these frames depending on task demands \cite{mou2004intrinsic, waller2007landmarks}, a capability that remains challenging for current AI systems \cite{anderson2018vln, chen2019touchdown}.

\subsection{Qualitative Spatial Reasoning}

The field of qualitative spatial reasoning (QSR) emerged from the need to represent and reason about spatial information without precise numerical coordinates \cite{cohn2001qualitative, renz2007qualitative}. The Region Connection Calculus (RCC) \cite{randell1992spatial} provides a formal framework for representing topological relationships between regions, while the Cardinal Direction Calculus \cite{frank1996qualitative, ligozat1998reasoning} handles directional relationships. These formalisms have been extended to handle temporal aspects \cite{muller1998qualitative, galton2000qualitative} and uncertainty \cite{cohn1997representing, schockaert2008fuzzy}.

Recent work has explored integrating QSR with neural networks \cite{chen2024spatialreasoning, mirzaee2021spartqa}, but significant challenges remain. The discrete, symbolic nature of QSR formalisms does not naturally align with the continuous representations learned by neural networks \cite{garcez2019neural, lamb2020graph}, and scaling these approaches to complex, real-world scenarios remains difficult \cite{davis2013ontologies, marcus2018deep}.

\subsection{Spatial Reasoning Benchmarks}

The evaluation of spatial reasoning in AI has evolved significantly over the past decade. Early benchmarks like bAbI \cite{weston2015towards} included simple spatial reasoning tasks but were quickly saturated by neural models \cite{sukhbaatar2015end, graves2016hybrid}. The CLEVR dataset \cite{johnson2017clevr} introduced visual spatial reasoning, requiring models to answer questions about synthetic 3D scenes. Subsequent work extended this paradigm to more realistic images \cite{hudson2019gqa, suhr2019nlvr2} and 3D environments \cite{savva2019habitat, kolve2017ai2thor}.

Text-based spatial reasoning benchmarks have also proliferated. SpartQA \cite{mirpuri2023spartqa} evaluates spatial reasoning through question answering, while StepGame \cite{shi2022stepgame} tests multi-hop spatial reasoning. RoomSpace2 \cite{li2025benchmarking} focuses on indoor spatial reasoning, and PlanQA \cite{PlanQA2025} evaluates planning in spatial contexts. However, these benchmarks often focus on abstract scenarios that do not capture the complexity of real-world spatial tasks.

Vision-language benchmarks have emerged to evaluate multimodal spatial reasoning. SpatialBench \cite{Xu2022AFF} assesses spatial understanding in VLMs, while GRASP \cite{Ma_2025_ICCV} uses grid-based environments. 3DSRBench \cite{SPARE3D2024} and Spatial457 \cite{Majumdar_2024_CVPR} evaluate 3D spatial reasoning. More recently, GeoAnalystBench \cite{zhang2025geoanalystbench} has focused on geospatial analysis tasks, and MapBench \cite{Chen_2024} evaluates map reading abilities.

Our work builds upon and extends this prior research. The comprehensive survey by Felicia et al. \cite{felicia2026from} provides a unified taxonomy of spatial AI agents and world models, identifying key capabilities and evaluation dimensions. SpatialOps operationalizes this framework by providing a large-scale benchmark that spans multiple spatial reasoning capabilities and is grounded in real-world applications.

\subsection{LLM Agents and Tool Use}

The development of LLM-based agents has opened new possibilities for spatial reasoning through tool use and environmental interaction \cite{yao2023react, schick2023toolformer, qin2023toolllm}. Agents can leverage external tools for computation \cite{gao2023pal, chen2022program}, information retrieval \cite{lewis2020rag, nakano2021webgpt}, and physical interaction \cite{ahn2022saycan, brohan2023rt2}. This paradigm has been particularly successful in code generation \cite{chen2021evaluating, li2022competition} and mathematical reasoning \cite{imani2023mathprompter, zhou2023solving}.

Benchmarks for LLM agents have emerged to evaluate these capabilities. AgentBench \cite{liu2023agentbench} provides a comprehensive evaluation across multiple environments, while WebArena \cite{zhou2023webarena} focuses on web-based tasks. SWE-bench \cite{jimenez2024swebench} evaluates software engineering capabilities, and Mind2Web \cite{deng2024mind2web} assesses web navigation. These benchmarks have revealed significant gaps between current LLM capabilities and human-level performance on complex, multi-step tasks.

\subsection{Graph Neural Networks for Spatial Data}

Graph Neural Networks (GNNs) have emerged as a powerful paradigm for processing spatial data \cite{kipf2017gcn, velickovic2018gat, hamilton2017graphsage}. By representing spatial relationships as graph structures, GNNs can capture complex dependencies that are difficult to model with traditional approaches \cite{bronstein2021geometric, battaglia2018relational}. Applications include traffic prediction \cite{li2018dcrnn, yu2018stgcn, wu2019graphwavenet}, point cloud processing \cite{qi2017pointnet, wang2019dynamic}, and molecular modeling \cite{gilmer2017neural, schutt2017schnet}.

Spatio-temporal GNNs extend this paradigm to dynamic spatial data \cite{jin2023stgnn, shehzad2024graphtransformers, bai2020agcrn}. These models have achieved state-of-the-art performance on tasks like traffic forecasting \cite{jiang2022gnn, bai2019stg2seq} and human motion prediction \cite{mao2019learning, li2020dynamic}. Recent work has explored integrating GNNs with LLMs \cite{chai2023graphllm, zhang2023graph}, potentially enabling more sophisticated spatial reasoning.

\section{The SpatialOps Benchmark}

\subsection{Design Principles}

SpatialOps is designed according to four core principles that distinguish it from existing benchmarks:

\textbf{Real-World Grounding:} Tasks are derived from documented industry use cases in telecommunications, utilities, government, and enterprise sectors. This grounding ensures that benchmark performance translates to practical capability \cite{raji2021ai, liao2023rethinking}.

\textbf{Comprehensive Coverage:} The benchmark spans twelve distinct categories of spatial reasoning, organized into three tiers of increasing complexity. This hierarchical structure enables fine-grained analysis of model capabilities \cite{srivastava2022beyond, liang2022holistic}.

\textbf{Controlled Difficulty:} All tasks are procedurally generated with configurable parameters, allowing precise control over difficulty levels. This enables systematic study of how performance degrades with increasing complexity \cite{press2022measuring, dziri2023faith}.

\textbf{Verifiable Ground Truth:} Every task includes a programmatic validator that computes the correct answer, ensuring 100\% ground-truth accuracy. This eliminates annotation errors that plague many benchmarks \cite{northcutt2021pervasive, klie2023annotation}.

\subsection{Task Taxonomy}

SpatialOps comprises twelve task categories organized into three tiers:

\textbf{Tier 1: Foundational Concepts} establishes basic spatial understanding:

\begin{itemize}
    \item \textbf{Coordinate Understanding (CU):} Tests comprehension of coordinate systems, including Cartesian coordinates, polar coordinates, and coordinate transformations \cite{newcombe2010picture, liben2006spatial}.
    
    \item \textbf{Geometric Reasoning (GR):} Evaluates knowledge of geometric shapes, properties (area, perimeter, angles), and spatial relationships (intersection, containment, overlap) \cite{clements2001geometry, battista2007development}.
    
    \item \textbf{Distance Computation (DC):} Assesses ability to calculate various distance metrics including Euclidean, Manhattan, Chebyshev, and geodesic distances \cite{deza2009encyclopedia, preparata1985computational}.
    
    \item \textbf{Topological Reasoning (TR):} Tests understanding of topological relationships (adjacency, connectivity, containment) independent of precise coordinates \cite{randell1992spatial, egenhofer1991reasoning}.
\end{itemize}

\textbf{Tier 2: Core Planning} requires algorithmic reasoning:

\begin{itemize}
    \item \textbf{Navigation and Pathfinding (NP):} Evaluates ability to find optimal paths using algorithms like A* \cite{hart1968formal}, Dijkstra \cite{dijkstra1959note}, and their variants \cite{koenig2004lifelong, likhachev2005anytime}.
    
    \item \textbf{Viewpoint and Visibility (VVA):} Tests determination of visibility and line-of-sight in environments with obstacles \cite{ghosh2007visibility, urrutia2000art}.
    
    \item \textbf{Pattern Recognition (PRA):} Assesses identification of spatial patterns, clusters, and anomalies in point distributions \cite{ester1996density, ankerst1999optics}.
    
    \item \textbf{Network Infrastructure (NI):} Evaluates analysis of network topologies, including connectivity, shortest paths, and failure analysis \cite{newman2018networks, barabasi2016network}.
\end{itemize}

\textbf{Tier 3: Advanced Optimization} involves complex multi-step reasoning:

\begin{itemize}
    \item \textbf{Constraint-Based Placement (CBP):} Tests placement of objects satisfying multiple spatial and logical constraints \cite{kumar1992algorithms, rossi2006handbook}.
    
    \item \textbf{Resource Allocation (RAO):} Evaluates optimization of resource placement to maximize coverage or minimize cost \cite{hochbaum1985best, vazirani2001approximation}.
    
    \item \textbf{Temporal-Spatial Reasoning (TSR):} Assesses reasoning about objects moving or changing over time \cite{muller1998qualitative, galton2000qualitative}.
    
    \item \textbf{Real Estate and Geospatial (RE):} Tests complex analysis of geospatial data including zoning, valuation, and site selection \cite{goodchild2007citizens, longley2015geographic}.
\end{itemize}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/figure2_task_distribution.png}
\caption{Distribution of tasks across categories and difficulty levels. Each category contains 501 tasks evenly distributed across easy, medium, and hard difficulty levels, totaling 6,012 tasks.}
\label{fig:distribution}
\end{center}
\end{figure}

\subsection{Dataset Composition}

The SpatialOps dataset comprises 6,012 tasks distributed evenly across the twelve categories and three difficulty levels. Each task is represented in a structured JSON format containing:

\begin{itemize}
    \item \textbf{Task ID:} Unique identifier encoding category, difficulty, and instance number
    \item \textbf{Question:} Natural language description of the spatial reasoning task
    \item \textbf{Context:} Structured spatial data (coordinates, graphs, constraints)
    \item \textbf{Ground Truth:} Verified correct answer computed by programmatic validator
    \item \textbf{Metadata:} Category, difficulty level, required reasoning steps
\end{itemize}

Difficulty levels are calibrated based on multiple factors: number of entities, complexity of constraints, required reasoning depth, and computational complexity of the optimal solution. Easy tasks require 1-2 reasoning steps, medium tasks require 3-5 steps, and hard tasks require 6+ steps or involve NP-hard subproblems.

\subsection{Industry Use Case Alignment}

A distinguishing feature of SpatialOps is its alignment with documented industry use cases. We surveyed spatial AI applications across four sectors:

\textbf{Telecommunications:} Network planning, fiber route optimization, coverage analysis, and infrastructure maintenance \cite{zhang2019deep, wang2020deep}.

\textbf{Utilities:} Asset management, outage prediction, load balancing, and infrastructure inspection \cite{chen2020application, wang2021power}.

\textbf{Government:} Urban planning, emergency response, resource allocation, and environmental monitoring \cite{batty2013big, bibri2017smart, UNHabitat2025}.

\textbf{Enterprise:} Real estate analysis, logistics optimization, site selection, and market analysis \cite{law2019take, fu2019real, DeSabbata2023}.

Each task category maps to specific industry applications, ensuring that benchmark performance reflects practical capability. This alignment is detailed in Appendix A.

\section{Evaluation Methodology}

\subsection{Multi-Faceted Metrics}

We propose five complementary metrics that provide a holistic assessment of model capabilities:

\textbf{Task Completion Rate (TCR)} measures the percentage of tasks where the model produces a valid, parsable response:
\begin{equation}
    TCR = \frac{|\{t \in T : \text{valid}(t)\}|}{|T|} \times 100\%
\end{equation}

\textbf{Accuracy (ACC)} measures the percentage of correct answers among completed tasks:
\begin{equation}
    ACC = \frac{|\{t \in T : \text{correct}(t)\}|}{|\{t \in T : \text{valid}(t)\}|} \times 100\%
\end{equation}

\textbf{Human-AI Latency Ratio (HLR)} quantifies speed-up compared to human professionals. We established baselines by measuring completion times for GIS analysts on representative task samples:
\begin{equation}
    HLR = \frac{\bar{T}_{human}}{\bar{T}_{AI}}
\end{equation}

\textbf{Operational Cost Savings (OCS)} estimates economic impact based on time savings and computational costs:
\begin{equation}
    OCS = (\bar{T}_{human} - \bar{T}_{AI}) \times R_{human} - C_{AI}
\end{equation}
where $R_{human}$ is the hourly rate and $C_{AI}$ is the API cost per task.

\textbf{Efficacy Score (ES)} provides a composite measure combining accuracy, reasoning quality (assessed via LLM-as-judge \cite{zheng2023judging}), and efficiency:
\begin{equation}
    ES = w_1 \cdot ACC + w_2 \cdot RQ + w_3 \cdot EFF
\end{equation}
where $w_1=0.5$, $w_2=0.3$, $w_3=0.2$ by default.

\subsection{Evaluation Protocol}

Models are evaluated using a standardized protocol:

\begin{enumerate}
    \item \textbf{Prompt Construction:} Each task is presented with a system prompt establishing the spatial reasoning context, followed by the task question and structured context data.
    
    \item \textbf{Response Generation:} Models generate responses with temperature=0 for reproducibility. Maximum token limits are set based on task complexity.
    
    \item \textbf{Answer Extraction:} Responses are parsed to extract the final answer using category-specific extractors.
    
    \item \textbf{Correctness Verification:} Extracted answers are compared against ground truth using appropriate matching criteria (exact match, numerical tolerance, set equivalence).
    
    \item \textbf{Reasoning Assessment:} For a stratified sample, reasoning chains are evaluated by GPT-4 using a 5-point rubric assessing logical coherence, spatial accuracy, and completeness.
\end{enumerate}

\section{Experiments}

\subsection{Models Evaluated}

We evaluate five leading LLMs representing the current state-of-the-art:

\begin{itemize}
    \item \textbf{GPT-5.2} (OpenAI): The latest iteration of the GPT series \cite{achiam2023gpt4}
    \item \textbf{Claude 3} (Anthropic): Emphasizes reasoning and safety \cite{anthropic2024claude}
    \item \textbf{Gemini 1.5} (Google): Multimodal with extended context \cite{anil2023palm}
    \item \textbf{Grok} (xAI): Designed for real-time information access
    \item \textbf{DeepSeek} (DeepSeek AI): Open-weight model with strong reasoning
\end{itemize}

\subsection{Main Results}

Table \ref{table:main} presents the main results across all models and metrics. GPT-5.2 achieves the highest overall Efficacy Score (78.4), followed by Claude 3 (73.8) and Gemini 1.5 (68.2). All models show significant performance degradation from Tier 1 to Tier 3 tasks, indicating that advanced spatial optimization remains challenging.

\begin{table}[t]
\caption{Main results on SpatialOps. ES: Efficacy Score, ACC: Accuracy, HLR: Human-AI Latency Ratio. Tier scores represent average accuracy within each tier.}
\label{table:main}
\begin{center}
\begin{tabular}{lccccccc}
\toprule
\textbf{Model} & \textbf{ES} & \textbf{ACC} & \textbf{HLR} & \textbf{Tier 1} & \textbf{Tier 2} & \textbf{Tier 3} \\
\midrule
GPT-5.2 & 78.4 & 72.5 & 847$\times$ & 85.2 & 72.1 & 60.3 \\
Claude 3 & 73.8 & 67.9 & 792$\times$ & 80.1 & 67.8 & 55.6 \\
Gemini 1.5 & 68.2 & 62.7 & 756$\times$ & 74.5 & 62.4 & 51.2 \\
Grok & 61.8 & 56.3 & 634$\times$ & 68.3 & 56.1 & 45.8 \\
DeepSeek & 56.0 & 49.9 & 589$\times$ & 62.1 & 50.2 & 40.5 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Category-Level Analysis}

Performance varies substantially across categories. All models perform well on Coordinate Understanding (CU) and Distance Computation (DC), with accuracies exceeding 80\% for top models. However, performance drops sharply for Constraint-Based Placement (CBP) and Resource Allocation (RAO), where even GPT-5.2 achieves only 52.3\% and 48.7\% accuracy respectively.

Navigation and Pathfinding (NP) reveals interesting patterns. While models can often identify correct paths in simple grids, they struggle with A* algorithm simulation on larger graphs, frequently making suboptimal choices or failing to properly account for heuristics.

\subsection{Ablation Studies}

\textbf{Impact of Prompt Detail:} Table \ref{table:ablation} shows that detailed prompts with explicit spatial reasoning instructions improve performance by 10-15\% across all models, suggesting that models benefit from structured guidance for spatial tasks.

\begin{table}[t]
\caption{Ablation study: Impact of prompt detail on Efficacy Score.}
\label{table:ablation}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Minimal Prompt} & \textbf{Detailed Prompt} \\
\midrule
GPT-5.2 & 68.2 & 78.4 \\
Claude 3 & 63.5 & 73.8 \\
Gemini 1.5 & 58.1 & 68.2 \\
Grok & 52.4 & 61.8 \\
DeepSeek & 46.3 & 56.0 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\textbf{Impact of Chain-of-Thought:} Explicit chain-of-thought prompting \cite{wei2022chain} improves performance on Tier 2 and Tier 3 tasks by 8-12\%, with larger gains on tasks requiring multi-step reasoning.

\textbf{Difficulty Scaling:} Performance degrades approximately linearly with difficulty level for Tier 1 tasks but shows steeper degradation for Tier 2 and 3, suggesting that complex spatial optimization poses qualitatively different challenges.

\subsection{Error Analysis}

We conducted detailed error analysis on 500 randomly sampled incorrect responses. The most common error types are:

\begin{enumerate}
    \item \textbf{Algorithmic Errors (34\%):} Incorrect application of spatial algorithms (e.g., A*, visibility computation)
    \item \textbf{Constraint Violations (28\%):} Solutions that violate stated spatial constraints
    \item \textbf{Numerical Errors (19\%):} Incorrect distance or coordinate calculations
    \item \textbf{Incomplete Reasoning (12\%):} Partial solutions that miss required components
    \item \textbf{Misinterpretation (7\%):} Misunderstanding of task requirements
\end{enumerate}

\section{Discussion}

\subsection{Implications for Spatial AI}

Our results reveal a significant gap between current LLM capabilities and the requirements of practical spatial AI applications. While models perform adequately on foundational tasks, their performance on advanced optimization problems remains far below human expert levels. This suggests that current architectures may lack the inductive biases necessary for robust spatial reasoning \cite{battaglia2018relational, bronstein2021geometric}.

The strong performance gains from detailed prompting indicate that models possess latent spatial reasoning capabilities that are not reliably activated by default. This aligns with findings on prompt sensitivity in other domains \cite{zhao2021calibrate, lu2022fantastically} and suggests that improved prompting strategies or fine-tuning could yield substantial gains.

\subsection{Comparison with Existing Benchmarks}

SpatialOps complements existing benchmarks by focusing on practical 2D spatial planning. While SpatialBench \cite{Xu2022AFF} evaluates VLM spatial understanding and GeoAnalystBench \cite{zhang2025geoanalystbench} focuses on GIS workflows, SpatialOps uniquely addresses the operational planning tasks critical for enterprise applications. The breadth of our benchmark, spanning twelve categories and three complexity tiers, enables more comprehensive assessment than narrower alternatives.

\subsection{Limitations}

Several limitations should be noted. First, our benchmark focuses on 2D spatial reasoning; extension to 3D would require substantial additional work. Second, while we ground tasks in industry use cases, the procedurally generated nature of tasks may not capture all real-world complexities. Third, our evaluation of reasoning quality relies on LLM-as-judge, which may have systematic biases \cite{zheng2023judging, wang2023large}.

\section{Conclusion}

We introduced SpatialOps, a comprehensive benchmark for evaluating 2D spatial planning and reasoning in Large Language Models. Our benchmark comprises 6,012 tasks across twelve categories, grounded in real-world industry applications and evaluated using a multi-faceted methodology. Extensive experiments reveal significant gaps in current model capabilities, particularly for advanced optimization tasks. We hope SpatialOps will serve as a valuable resource for tracking progress and guiding research toward more spatially capable AI systems.

\bibliographystyle{plain}
\bibliography{references_full}

\end{document}
